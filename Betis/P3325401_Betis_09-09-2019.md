**Title: Scheduling Beyond CPUs for HPC**

**Publication: Proceeding HPDC '19 Proceedings of the 28th International
Symposium on High-Performance Parallel and Distributed Computing Pages 97-108**

The ACM International Symposium on High-Performance Parallel and Distributed
Computing (HPDC) is the premier annual conference for presenting the latest
research on the design, implementation, evaluation, and the use of parallel and
distributed systems for high-end computing.

**Author：Yuping Fan, Zhiling Lan, Paul Rich, William E. Allcock, Michael E.
Papka, Brian Austin, and David Paul. 2019. DOI:**
<https://doi.org/10.1145/3307681.3325401>

**Paper Review**

-   Research Background

    -   Mainly focused on scheduling Jobs on HPC based on Multi-objective
        selection

    -   Comparing different scheduling algorithms on existing HPC systems

-   Problem to Solve

    -   Most of schedulers are based on optimization for CPU and single
        objective but this paper is choosing to take a naïve approach to solve
        the scheduling problem in multi objective approach.

-   Key Design and Algorithm Proposed

    -   Naïve method

        -   Based on scheduling FCFS scheduling algorithm

        -   Good for node utilization

        -   Not applicable for burst utilization

    -   Constraint Method

        -   Better than naïve for CPU utilization (node)

        -   Same issue over burst utilization

    -   Weighted Method and Bin Packing

        -   They suffer from same issue due to the regression to one dimension

    -   Pareto Set

        -   Multi objective method which was the main concern of this paper

        -   To utilize for both CPU and Burst based on removing dominate set

    -   Window-based scheduling

        -   Their approach is to take a window size of jobs in HPC and randomly
            select each job from that set to be run

        -   Issues are starvation which they address with setting an upper bound

        -   Other issue can be large or small window size which could ended up
            in worst performance comparing to other schedulers

        -   Equation are trying to maximize the node and burst utilization by
            applying the simple multiplication over nodes and burst

        -   Impact of windows sizes can be grown exponentially if it wouldn’t be
            controlled.

-   Major Contribution

    -   The authors tried to solve these issues by creating a children of
        scheduler window randomly and choose the best one based on pareto
        principle

-   Major limitation

    -   One issue over here is pareto principle doesn’t give a solution so
        overall you can have multiple solution which can delay or increase the
        complexity

-   Something you don’t understand

    -   In evaluation section the authors mentioned that in this method overall
        performance was increased by average of 10 to 15 percent however in
        figure 13 they sketch it as if this method would solve the scheduling
        problem and the increase rate is 100% ( perfect figures for their
        scheduler??)

-   Your view on the research domain/topic/approach/data/solution (positive or
    negative)

    -   The authors view of multi object scheduling algorithm for HPC is good
        and their approach is fair however it is naïve based approach and we can
        improve on the randomization of window sizes and even replace it with
        graph analysis of each job batch.

    -   In order to increase the dimension to third or fourth we need to come up
        with a better solution other than randomization

    -   Each job batch can be represented as a graph based on resources then we
        can fill the windows based on similar batches.

    -   Jobs can be replaced with workflows in future case
