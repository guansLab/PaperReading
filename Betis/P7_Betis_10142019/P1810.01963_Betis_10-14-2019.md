**Title: Learning Scheduling Algorithms for Data Processing Clusters**

**Publication: Proceeding SIGCOMM '19 Proceedings of the ACM Special Interest
Group on Data Communication Pages 270-288**

**Beijing, China — August 19 - 23, 2019 ACM New York, NY, USA ©2019 ISBN:
978-1-4503-5956-6 doi\>10.1145/3341302.3342080**

The Third Asia-Pacific Workshop on Networking (APNet'19) will be co-located with
ACM SIGCOMM 2019 and will take place August 17-18, 2019, at the Jinlongtan Hotel
in Beijing, China. APNet aims to bring together the very best researchers in
computer networking and systems across the Asia-Pacific region to a live forum
discussing and debating innovative ideas at their early stages. The mission of
APNet is that promising but not-yet-mature ideas can receive timely feedback
from experienced researchers, shaping them into major conferences such as
SIGCOMM, SOSP, OSDI, NSDI, MobiCom, CoNEXT and so on.

**Author: Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan,
Mohammad Alizadeh MIT Computer Science and Artificial Intelligence Laboratory,
Zili Meng Tsinghua University**

-   Research Background

    -   Efficiently scheduling data processing jobs on distributed compute
        clusters requires complex algorithms

    -   RL neural networks are yet to be investigated in scheduling problem

    -   The authors attend to solve this problem by “Decima”

-   Problem to Solve

    -   Using RL and DAG model the authors tried to introduce a new way to
        calculate the optimal solution for large network scheduling

    -   They used the spark and alibabas data to test their method

-   Key Design and Algorithm Proposed

    -   Dependency-aware task scheduling

    -   Setting the right level of parallelism

    -   The DAG Scheduling Problem in Spark

        -   Scalable state information processing

        -   Huge space of scheduling decisions

        -   Training for continuous stochastic job arrivals

    -   Design

        -   The graph embedding takes as input the job DAGs whose nodes carry a
            set of stage attributes (e.g., the number of remaining tasks,
            expected task duration, etc.), and it outputs three different types
            of embeddings: (1) per-node embeddings, which capture information
            about the node and its children (containing, e.g., aggregated work
            along the critical path starting from the node); (2) per-job
            embeddings, which aggregate information across an entire job DAG
            (containing, e.g., the total work in the job); and (3) a global
            embedding, which combines information from all per-job embeddings
            into a cluster-level summary (containing, e.g., the number of jobs
            and the cluster load).

        -   Per-node embeddings

        -   Per-job and global embeddings

-   Major Contribution

    -   Using Adams method in RL and applying reward as multi objective variant
        they managed to train the system faster and more efficient for short
        jobs

-   Major limitation

    -   May still starve the large jobs

-   Something you don’t understand

    -   The introduce limitation for solving the starvation however in large
        scale systems that limitation can cause additional overhead to system
        itself

-   Your view on the research domain/topic/approach/data/solution (positive or
    negative)

    -   It’s a good read and useful paper specially on RL and neural network it
        is well defined and resourceful.
