**Title: TETRIS: Scalable and Efficient Neural Network Acceleration with 3D
Memory**

**Publication: Proceeding ASPLOS '17 Proceedings of the Twenty-Second
International Conference on Architectural Support for Programming Languages and
Operating Systems Pages 751-764**

**Xi'an, China — April 08 - 12, 2017 ACM New York, NY, USA ©2017 table of
contents ISBN: 978-1-4503-4465-4 doi\>10.1145/3037697.3037702**

**· Newsletter ACM SIGARCH Computer Architecture News - Asplos'17 Volume 45
Issue 1, March 2017 Pages 751-764 ACM New York, NY, USA table of contents
doi\>10.1145/3093337.3037702**

**· Newsletter ACM SIGPLAN Notices - ASPLOS '17 Volume 52 Issue 4, April 2017
Pages 751-764 ACM New York, NY, USA table of contents
doi\>10.1145/3093336.3037702**

**Author: Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, Christos Kozyrakis
Stanford University, Stanford, CA, USA**

Research Background

-   Using 3D memory HMC, HBM and new method of scheduling for NN authors attempt
    to solve the bandwidth and power consumption problem

Problem to Solve

-   Using 3D memory in NN can be beneficial however, the complexity of assigning
    Pes to memory location and finding the optimal solution can be challenging,
    in this paper authors intent to solve the problem of scheduling and
    optimization of Pes over 3D memory

Key Design and Algorithm Proposed

3D memory Introduction  

-   Is only beneficial with Pes over 200

-   Uses logical Die for creating a mesh network over DDRx banks

-   Can be optimized by exhausting search algorithm

-   Applicable for Parallel processing and Large numbers of PEs

-   Bandwidth can be increased from 25GBs to 150GBs

-   Efficient power consumption

Tetris Scheduling and Partitioning

Dataflow scheduling

Mapping problem row stationary method was used

-   Row stationary maps the 1D convolution primitive onto a single PE to utilize
    the PE register file for local data reuse

Provides an optimal solution for mapping problem

To solve ordering problem bypass ordering method proposed

§  (➊). For each ifmap chunk, the ofmaps are streamed directly from DRAM into
the register files

§  (➋). Since each ifmap-ofmap pair uses a different 2D filter, the filters have
little reuse outside of batching and are also streamed directly from DRAM

§  (➌). The register files are used exactly the same way as row stationary
dataflow dictates: the ifmaps in the global buffer will be streamed in

§  (➍) to process with the ofmaps and filters stored in the register files

§  (➎). Recall that the numbers of ifmaps, ofmaps, and the batch size are Ni ,
No, Nb, respectively.

-   Total number of memory accessed :

§  ADRAM = 2× NbNoSo ×ti + NbNiSi + NoNiSw ×tb

§  Fmap (image) partitioning: If the fmap is large (e.g., 112 × 112), we can
partition it into smaller tiles

§  Output partitioning: As each layer usually has multiple ofmaps, we can
partition the ofmaps across vaults

§  Input partitioning: Similar to output partitioning, partition the Ni ifmaps
across vaults.

-   To partitioning across multiple stacks the authors used the same methodology

Major Contribution

-   By using 3D memory architecture and hybrid scheduling method the achieved a
    better performance to run NN on large number of PEs

Major limitation

-   Needs more Pes to achieve a better performance

-   Applicable to special hardware

Something you don’t understand

Your view on the research domain/topic/approach/data/solution (positive or
negative)

-   Method over scheduling and creating mesh network is a potential candidate
    for our project

 

 

 
