

### Title: AI fairness 360: an extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias

### Publication: eprint arXiv:1810.01943 
arXiv is an online digital archive for electronic preprints of scientific papers

### Author：IBM Research, including Rachel K. E. Bellamy, Kuntal Dey, Michael Hind et al.
IBM Research is one of the world’s largest and most influential corporate research labs

Rachel K. E. Bellamy is the principal researcher and manager of human-AI collaboration at Thomas J. Watson Research Center, Yorktown Heights, NY USA. Recent research and design work has included work on cognitive user interfaces, studies of cognitive bias, design for mobile applications for hand-drawn computing, suport for business user's working with big data and interfaces for multi-factor mobile authentication.

Kuntal Dey is a Senior Software Engineer (Research) in IBM Research India, and is currently working with the Artificial Intelligence Engineering division in IBM Research India.

Michael Hind is a Distinguished Research Staff Member in the IBM Research AI department in Yorktown Heights, New York. His current research passion is in the general of area of Trusted AI, focusing on the fairness, explainability, transparency, and reliability of the construction of AI systems.

### Paper Review
- Research Background
  1. Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. 
  2. Also fairness is a complex and multi-faceted concept that depends on context and culture, in some cases different definitions produce entirely different outcomes.
  3. In recent years several open source repositories have been developed to provide various levels of functionality in learning fair AI models.

- Problem to Solve
  1. It is impossible to satisfy all definitions of fairness at the same time and difficult to decide what is the right fairness metric.
  2. In addition to the multitude of fairness definitions, different bias handling algorithms address different parts of the model life-cycle, and understanding each research contribution, how, when and why to use it is challenging even for experts in algorithmic fairness.
  3. although fairness research is a very active area, and there are several libraries to provide various functionalities in this field, many of these deal only with bias detection, and provide no techniques for mitigating such bias.

- Key Design and Algorithm Proposed
  - This paper proposed a system named AIF360, which is the first system to bring together in one open source toolkit: bias metrics, bias mitigation algorithms, bias metric explanations, and industrial usability. And offered a rigorous architectural design focused on extensibility, usability, explainability, and ease of benchmarking that goes beyond the existing work.
  - The figure below shows the generic algorighm for bias mitigation.

    
- Major Contribution
To introduce a new open source Python toolkit to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.
  1. Offering an extensible architecture that incorporates dataset representations and algorithms for bias detection, bias mitigation, and bias metric explainability.
  2. Providing an empirical evaluation that demonstrates how AIF360 can be used for scientific comparisons of bias metrics and mitigation algorithms.
  3. Giving a design of an interactive web experience to introduce users to bias detection and mitigation techniques.
  
- Major limitation

  

- Something you don’t understand

  

- Your view on the research domain/topic/approach/data/solution  (positive or negative)
