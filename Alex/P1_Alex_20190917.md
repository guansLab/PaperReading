

### Title: AI fairness 360: an extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias

### Publication: eprint arXiv:1810.01943 
arXiv is an online digital archive for electronic preprints of scientific papers

### Author：IBM Research, including Rachel K. E. Bellamy, Kuntal Dey, Michael Hind et al.
IBM Research is one of the world’s largest and most influential corporate research labs

Rachel K. E. Bellamy is the principal researcher and manager of human-AI collaboration at Thomas J. Watson Research Center, Yorktown Heights, NY USA. Recent research and design work has included work on cognitive user interfaces, studies of cognitive bias, design for mobile applications for hand-drawn computing, suport for business user's working with big data and interfaces for multi-factor mobile authentication.

Kuntal Dey is a Senior Software Engineer (Research) in IBM Research India, and is currently working with the Artificial Intelligence Engineering division in IBM Research India.

Michael Hind is a Distinguished Research Staff Member in the IBM Research AI department in Yorktown Heights, New York. His current research passion is in the general of area of Trusted AI, focusing on the fairness, explainability, transparency, and reliability of the construction of AI systems.

### Paper Review
- Research Background
  1. Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. 
  2. Also fairness is a complex and multi-faceted concept that depends on context and culture,in some cases different definitions produce entirely different outcomes.
  3. In recent years several open source libraries or repositories have been developed to provide various levels of functionality in learning fair AI models.

- Problem to Solve
It is difficult to decide what is the right fairness metric.
researchers have shown that it is impossible to satisfy all definitions of fairness at the same time.
although fair- ness research is a very active field, clarity on which bias metrics and bias mitigation strategies are best is yet to be achieved.
In addition to the multitude of fairness definitions, differ- ent bias handling algorithms address different parts of the model life-cycle, and understanding each research contri- bution, how, when and why to use it is challenging even for experts in algorithmic fairness.
 fairness scientific community and AI practitioners need clarity on how to proceed.
 Currently the burden is on ML and AI developers, as they need to deal with questions such as “Should the data be debiased?”, “Should we create new classifiers that learn unbiased models?”, and “Is it better to correct predictions from the model?”

- Key Design and Algorithm Proposed
AIF360 is the first system to bring together in one open source toolkit: bias metrics, bias mitigation algorithms, bias metric explanations, and industrial usability.

- Major Contribution
To address these issues we have created AI Fairness 360 (AIF360), an extensible open source toolkit for detect- ing, understanding, and mitigating algorithmic biases. The goals of AIF360 are to promote a deeper understanding of fairness metrics and mitigation techniques; to enable an open common platform for fairness researchers and indus- try practitioners to share and benchmark their algorithms; and to help facilitate the transition of fairness research al- gorithms to use in an industrial setting.
To introduce a new open source Python toolkit to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.
  1. Offering an extensible architecture that incorporates dataset representations and algorithms for bias detection, bias mitigation, and bias metric explainability.
  2. Providing an empirical evaluation that demonstrates how AIF360 can be used for scientific comparisons of bias metrics and mitigation algorithms.
  3. Giving a design of an interactive web experience to introduce users to bias detection and mitigation techniques.
  
- Major limitation

  

- Something you don’t understand

  

- Your view on the research domain/topic/approach/data/solution  (positive or negative)
