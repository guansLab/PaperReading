### Title: Improved Training of Wasserstein GANs https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf

### Publication: NIPS

### Author：Ishaan Gulrajani et al.


### Paper Review
- Research Background

  GANs are able to generate data close to original data, but it always suffer from instability problem. Many researchers tried to find some methods to stabilize the training procedure of GANs.

- Problem to Solve

  Wasserstein GANs are proposed solve instability problem of GANs, but it also sometimes suffer from instability problem.

- Key Design and Algorithm Proposed

  They penalize the norm of gradient of the critic w.r.t. the input instead of clipping weight.

- Major Contribution

  1. They solve the gradient vanishing and exploding problems that may happen in WGANs with original weight clipping.
  2. They compare their method with other three GANs algorithms, demonstrating their method's advantage.

- Major limitation

  They only use inception score to test whether a generative model can generate better images.

- Something you don’t understand

  I don't know what does their 200 architectures mean.

- Your view on the research domain/topic/approach/data/solution  (positive or negative)

  1. Improving training procedure of GANs to prevent divergence is promising.
  2. Identifying where the problem exists and thus using another method to solve the problem is a good method.
  3. They only use LSUN and CIFAR-10 dataset in their paper, with some other datasets with complex images unexplored.

- The image which is important

![Image of Improved WGAN](Improved%20WGAN.jpg)
