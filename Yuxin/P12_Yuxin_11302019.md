### Title: Improved Techniques for Training GANs https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf

### Publication: NIPS

### Author：Tim Salimans et al.

![Image of Improved GAN](Improved%20GAN.jpg)

### Paper Review
- Research Background

  GANs can produce good samples, and training GANs primarily uses gradient descent with a cost function to find the optimum. However, these methods cannot converge to a Nash equilibrium.

- Problem to Solve

  Modifying parameters of generator can reduce the value of the cost function of the generator, but it can increase the value of the cost function of the discriminator. Also, modifying parameters of discriminator can reduce the value of the cost function of the discriminator but increases the value of the cost function of the generator.

- Key Design and Algorithm Proposed

  They propose five methods, feature matching, minibatch discrimination, historical averaging, one-sided label smoothing, and virtual batch normalization, to help GANs to converge to an equilibrium.

- Major Contribution

  1. They prevent GANs to be untrainable.
  2. With these methods, they achieve the state-of-the-art results using datasets in computer vision.

- Major limitation

  Even though they propose many methods and explain those methods to help GANs to converge, they neither mention how to set or fine-tune those parameters, nor mention why they set such parameters in their works.

- Something you don’t understand

  I don't know what is the feature represents in feature matching.

- Your view on the research domain/topic/approach/data/solution  (positive or negative)

  1. I think helping GANs to converge is promising because in practice GANs are hard to train and converge to the global optimum.
  2. I think using some existing methods to improve training GANs is good.
  3. Datasets used by them are some common image datasets used in computer vision.
